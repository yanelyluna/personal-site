---
title: Distribuciones de pérdida
author: ''
date: '2021-05-27'
slug: []
categories: []
tags: []
subtitle: ''
excerpt: ''
images: ~
series: ~
layout: single
---

```{r setup, echo=FALSE,warning=FALSE,message=FALSE}
library(ggplot2)
library(tidyr)
library(dplyr)
library(knitr)
library(insuranceData)
library(fitdistrplus)
library(actuar)
knitr::opts_chunk$set(echo=FALSE,warning=FALSE,message=FALSE,fig.align = "center",fig.height = 4)
```

En el modelo de riesgo colectivo debemos especificar dos distribuciones de probabilidad: una para el número de reclamaciones (frecuencia) y otra para el monto de las reclamaciones (severidad). En este documento se presentan algunas distribuciones, conocidas en este contexto como **distribuciones de pérdida**, sus propiedades y sugerencias sobre cómo y cuándo usarlas.

## Para comenzar: Variables aleatorias en `R`

La paquetería `stats`de `R` (que ya viene precargada al iniciar sesión) proporciona cuatro funciones básicas para cada distribución de variables aleatorias. Dichas funciones comienzan con `d`, `p`, `q` o `r`; el resto del nombre depende de la distribución con la que queremos trabajar. Por ejemplo, para la distribución Poisson usamos el sufijo `pois`:

-   `dpois(x,lambda)` Calcula el valor de la función de densidad Poisson con parámetro `lambda` evaluada en `x`.

-   `ppois(x,lambda)` Calcula el valor de la función de distribución Poisson con parámetro `lambda` evaluada en `x`, es decir $P(X\leq x), X\sim Poisson(\lambda)$.

-   `qpois(p,lambda)` Calcula el cuantil `p` de la distribución Poisson con parámetro `lambda`, es decir, calcula `x` como el número más pequeño tal que $P(X\leq x) \geq p$.

-   `rpois(n,lambda)` Genera `n` observaciones de una v.a. de una distribución Poisson con parámetro `lambda`.

El número de parámetros de reciben las funciones depende de los parámetros de cada distribución, pero en general siguen la estructura anterior.

Para las distribuciones presentadas en este documento:

|                   |                                                                         |
|-------------------|-------------------------------------------------------------------------|
| Distribución      | Sufijo                                                                  |
| Poisson           | `pois`                                                                  |
| Binomial Negativa | `nbinom`                                                                |
| Binomial          | `binom`                                                                 |
| Exponencial       | `exp`                                                                   |
| Gamma             | `gamma`                                                                 |
| Pareto            | `pareto` (no se encuentra en la paquetería `stats` pero sí en `actuar`) |
| Lognormal         | `lnorm`                                                                 |

: Sufijos para distribuciones en `R`

## Distribuciones para la frecuencia

Cuando queremos modelar el número de reclamaciones, debemos considerar distribuciones de probabilidad para variables que tomen valores en los enteros no negativos, entre otras características propias del evento de interés. En esta sección se proponen dos familias paramétricas populares para la frecuencia de reclamaciones.

### Poisson

Si $Y \sim Poisson(\lambda)$ ($\lambda > 0$), $$f_Y(y) = \frac{\lambda^y}{y!}e^{-\lambda}$$ $y=0,1,2,...$

**Propiedades:**

-   $E(Y) = \lambda$

-   $Var(Y) = \lambda$

-   $M_Y(t) = e^{\lambda(e^t - 1)}$

-   Para una muestra de v.a.i.i.d. $Y_1,Y_2,...,Y_n$ con distribución Poisson($\lambda$), el estimador máximo verosímil de $\lambda$ es $\hat{\lambda} = \overline{Y}$.

Es importante notar que en esta distribución, la esperanza es igual a la varianza ($\lambda$) por lo que es una buena opción cuando la muestra tiene una media y varianza observada muy cercanas (el cociente de la media entre la varianza es cercano a 1).

```{r pois,fig.cap="Funciones de densidad Poisson para distintos valores de lambda."}
x<-0:30
lambda=c(4,6,8)
set.seed(92)
df_pois <- data.frame(x=x, lambda1=rep(lambda,each=length(x)),
                      fx = c(dpois(x,lambda[1]),dpois(x,lambda[2]),dpois(x,lambda[3]))) %>%
  mutate(lambda1=as.factor(lambda1))

ggplot(df_pois,aes(x=x,y=fx,color=lambda1)) +
  geom_point() +
  labs(color="lambda") +
  facet_wrap(~lambda1) +
  ggtitle("Densidad Poisson(lambda)") +
  ylab("fx") +
  theme_bw()

n <- 300
```

Podemos simular varias muestras Poisson (cada una de tamaño $n=$ `r n`) con distintos parámetros:

```{r m_pois,fig.cap="La línea roja en cada gráfica muestra el valor estimado de lambda."}
set.seed(939)
muestra1 <- rpois(n,lambda[1])
muestra2 <- rpois(n,lambda[2])
muestra3 <- rpois(n,lambda[3])
muestra_pois <- data.frame(lambda2=c(rep(lambda[1],n),rep(lambda[2],n),rep(lambda[3],n)),
                           y_obs=c(muestra1,muestra2,muestra3)) %>%
  mutate(lambda2=as.factor(lambda2))
mean_m <-data.frame(lambda2=lambda,
                    media=c(mean(muestra1),mean(muestra2),mean(muestra3)))
ggplot(muestra_pois,aes(x=y_obs,fill=lambda2)) +
  geom_histogram() + 
  geom_vline(aes(xintercept = media),mean_m,color="red",size=1.5) +
  facet_wrap(~lambda2) +
  ggtitle("Distribución de las muestras aleatorias") +
  theme_bw()
```

Parámetros estimados:

```{r par_pois}
colnames(mean_m) <- c("lambda", "lambda estimada (media)")
kable(mean_m, align = 'c')
```

**Ejemplo 1**

Estimación de $\lambda$ usando los datos de [`ClaimsLong`](https://teoria-del-riesgo.netlify.app/posts/2021-03-18-datasets/#claimslong-claims-longitudinal):

```{r claimslong, echo=TRUE,fig.cap="Distribución empírica (azul) y distribución Poisson ajustada (rojo)."}
#Cargamos los datos
library(insuranceData)
data("ClaimsLong")

#Numero de reclamaciones por póliza durante el primer año.
Y1 <- ClaimsLong$numclaims[ClaimsLong$period==1]

#Función de distribución empírica 
F_emp <- ecdf(Y1)

#Lambda estimada
(lambda1 <- mean(Y1))

```

```{r,include=FALSE}
#Función de distribución empírica y distribución con el parámetro estimado
ggplot(data.frame(x=0:max(Y1),y=F_emp(0:max(Y1))),aes(x=x,y=y)) +
  geom_point()+
  geom_step(color="blue") +
  geom_step(aes(x=0:max(Y1),y=ppois(0:max(Y1),lambda = lambda1)),color="red") +
  ylab("F(y)") + xlab("y_obs") + ggtitle("Funciones de distribución") +
  theme_bw()
```

**Observación**: La media del número de reclamaciones ($\hat{\lambda}$) es `r mean(Y1)` mientras que la varianza observada es `r var(Y1)`, por lo que una ajustar una distribución Poisson no es lo más recomendable.

Si $Y_i$ representa el número de reclamaciones de la póliza $i$, en el ejemplo anterior estariamos suponiendo que cada póliza (observación) tiene exposición 1. Cuando cada póliza tiene un valor de exposición diferente, digamos $w_i$, debemos tomarlo en cuenta para la estimación de $\lambda$. En tal caso $Y_i \sim Poisson(\lambda\cdot w_i)$ y el estimador máximo verosímil de $\lambda$ es $$\hat{\lambda}=\frac{\sum_{i=1}^n Y_i}{\sum_{i=1}^n w_i}$$

**Ejemplo 2**

Usando el conjunto de datos [`dataCar`](https://teoria-del-riesgo.netlify.app/posts/2021-03-18-datasets/#datacar-data-car) donde cada póliza tiene asociada su exposición, estimamos el valor de $\lambda$:

```{r dataCar, echo=TRUE}
#Cargamos los datos
data(dataCar)
#Variables del número de reclamaciones y de exposición
Y <- dataCar$numclaims
W <- dataCar$exposure
#Lambda estimada
(lambda_hat <- sum(Y)/sum(W))
```

```{r dataCar2}
#head((Y/W)[Y!=0])
#(var_hat <- sum((Y-lambda_hat*W)^2)/sum(W))
#(phi <- var_hat/lambda_hat) # debe ser aprox 1

```

### Binomial Negativa

Si $Y \sim BinNeg(r,p)$ entonces $$f_Y(y) = \binom{r+y-1}{y}p^r(1-p)^y$$ para $y=0,1,2,...$

**Propiedades;**

-   $E(Y) = \frac{r(1-p)}{p}$

-   $Var(Y) = \frac{r(1-p)}{p^2}$

-   $E(Y) < Var(Y)$

-   $M_Y(t) = \left(\frac{p}{1-(1-p)e^t}\right)^r$

-   Usando el método de momentos, los estimadores de $r$ y $p$ son: $$\hat{r}_{mm}= \frac{\overline{Y}}{\hat{\sigma}^2-\overline{Y}},$$ $$\hat{p}_{mm} = \frac{\overline{Y}}{\hat{\sigma}^2}$$ donde $\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n (Y_i-\overline{Y})^2$. **NOTA:** En `R` la función `var()` calcula $\frac{1}{n-1}\sum_{i=1}^n (Y_i-\overline{Y})^2$ por lo que obtenemos $\hat{\sigma}^2$ como `var(Y)*(n-1)/n`.

-   Cuando $r$ es conocida, el estimador máximo verosímil de $p$ es $$\hat{p}_{mv} = \frac{r}{\overline{Y}+r}$$

```{r binneg,fig.cap="Funciones de densidad Binomial Negativa con distintos parámetros."}
x<-0:30
r <- c(5,4,3)
p <- c(0.6,0.5,0.2)
set.seed(99)
df_bn <- data.frame(x=x,r1 = rep(r,each=length(x)),
                    p1=rep(p,each=length(x)),
                    y=c(dnbinom(x,r[1],p[1]),dnbinom(x,r[2],p[2]),dnbinom(x,r[3],p[3]))) %>%
  mutate(r1 = as.factor(r1),p1= as.factor(p1))

ggplot(df_bn,aes(x=x,y=y,color=p1)) +
  geom_point() +
  facet_wrap(~r1) +
  ggtitle("Densidad BinomNeg(r,p)") +
  labs(color="Parámetros") + ylab("fx") +
  theme_bw()

```

**Ejemplo 3**

Estimamos el valor de los parámetros con los que simulamos la muestra usando el método de momentos y por máxima verosimilitud con el paquete `fitdistrplus`.

```{r ej3,echo=TRUE}
n <- 300 #Tamaño de muesta
# Parámetros
r <- 5
p <-0.6

# Muestra simulada con los parámetros elegidos
set.seed(43)
y <- rnbinom(n,size = r,prob = p)

# Estimación de los parámetros con el método de momentos
(r_mm <- (mean(y)^2)/(var(y)*(n-1)/n-mean(y))) 
(p_mm <- mean(y)/(var(y)*(n-1)/n)) 

# Usando la paquetería fitdistrplus
fd_bn <- fitdist(y,distr="nbinom",method = "mle")
fd_bn$estimate

```

La función `fitdist` estima los parámetros de la distribución que maximizan la log-verosimilitud. Sin embargo, para la distribución binomial negativa estima la esperanza de la distribución ($\mu = \frac{r(1-p)}{p}$) por lo que podemos obtener un valor estimado de $p$ como $$\hat{p}_{mv} = \frac{\hat{r}}{\hat{r}+\mu}$$

```{r mv_bn,echo=TRUE}
mu <- as.numeric(fd_bn$estimate[2])

#Valor estimado de r por máxima verosimilitud
(r_mv <- as.numeric(fd_bn$estimate[1]))

#Valor estimado de p por máxima verosimilitud
(p_mv <- r_mv/(r_mv + mu))
```

```{r tabla_bn}
kable(data.frame(par=c(r,p),mm=c(r_mm,p_mm),mv=c(r_mv,p_mv)),
      col.names = c("Valor de los parámetros","Estimación por momentos","Estimación por MV"))
```

**Observación**: A diferencia de la distribución Poisson, en la distribución Binomial Negativa la esperanza es menor a la varianza, por lo que puede ser una mejor opción para los datos del Ejemplo 1.

**Ejemplo**

Usando los mismos datos del Ejemplo 1.

```{r clamislong2, echo=TRUE,fig.cap="Distribución empírica (azul) y distribución binomial negativa ajustada (rojo)."}
# Y1 <- Número de reclamaciones por póliza durante el primer año de vigencia
n1 <- length(Y1)
# Parámetros estimados con el método de momentos
(r_est <- mean(Y1)^2)/(var(Y1)*(n1-1)/n1-mean(Y1))
(p_est <- mean(Y1)/(var(Y1)*(n1-1)/n1))

```

```{r,include=FALSE}
#Función de distribución empírica y distribución con el parámetro estimado
ggplot(data.frame(x=0:max(Y1),y=F_emp(0:max(Y1))),aes(x=x,y=y)) +
  geom_point()+
  geom_step(color="blue") +
  geom_step(aes(x=0:max(Y1),y=pnbinom(0:max(Y1),size = r_est,prob=p_est)),color="red") +
  ylab("F(y)") + xlab("y_obs") + ggtitle("Funciones de distribución") +
  theme_bw()
```

### Binomial

Si $Y \sim Binomial(m,p)$ entonces $$f_Y(y) = \binom{m}{y}p^y(1-p)^{m-y}$$ para $y=0,1,2,...,n$.

**Propiedades:**

-   $E(Y) = mp$

-   $Var(Y) = mp(1-p)$

-   $Var(Y) < E(Y)$

-   $M_Y(t) = (1-p+pe^t)^n$

-   Para una muestra $Y_i$ de v.a.i.i.d. Binomial$(m,p)$, los estimadores por el método de momentos son $$\hat{p}_{mm} = 1-\frac{\hat{\sigma}^2}{\overline{Y}}$$ $$\hat{m}_{mm} = \frac{\overline{Y}^2}{\overline{Y}-\hat{\sigma}^2}$$

-   Cuando el parámetro $m$ es conocido, el estimador máximo verosímil de $p$ es $$\hat{p}_{mv}=\frac{\overline{Y}}{m}$$

```{r binom,fig.cap="Función de densidad Binomial(m,p) para distintos parámetros m y p."}
x<-0:30
m <- c(9,15,30)
p <- c(0.3,0.5,0.7)

set.seed(39)
df_binom <- data.frame(x=x, m1=rep(m,each=length(x)), p1 = rep(p,each=length(x)),
                      fx = c(dbinom(x,m[1],p[1]),dbinom(x,m[2],p[2]),dbinom(x,m[3],p[3]))) %>%
  mutate(m1=as.factor(m1),p1= as.factor(p1))

ggplot(df_binom,aes(x=x,y=fx,color=p1)) +
  geom_point() +
  labs(color="p") +
  facet_wrap(~m1) +
  ggtitle("Densidad Binomial(m,p)") + ylab("fx") +
  theme_bw()

```

**Ejemplo 4**

Simulamos una muestra y estimamos el parámetro `p` suponiendo que conocemos el valor de `m`.

```{r ej4, echo=TRUE}
n <- 300 #Tamaño de muesta
# Parámetros
m <- 5
p <-0.6

# Muestra simulada con los parámetros elegidos
set.seed(43)
y <- rbinom(n,size = m,prob = p)

# Estimación p con el método de momentos
(p_mm <- 1-(var(y)*(n-1)/n)/mean(y)) 

# Estimación de p por máxima verosimilitud
(p_mv <- mean(y)/m)
```

```{r tabla_binom}
kable(data.frame(par=c(p),mm=p_mm,mv=p_mv),
      col.names = c("Valor del parámetro","Estimación por momentos","Estimación por MV"))
```

## Distribuciones para la severidad

### Exponencial

Si $Y\sim$ Expoencial$(\lambda)$ entonces, $$f_Y(y) = \lambda e^{-\lambda y}$$ con $\lambda > 0$ para $y>0$.

```{r exp, fig.cap="Función de densidad Exponencial con distintos parámetros."}
ggplot(data.frame(x = c(0, 4)), aes(x = x)) +
        stat_function(fun = dexp, args = 3,
                      aes(colour = "lambda = 6")) +
        stat_function(fun = dexp, args = 2,
                      aes(colour = "lambda = 2")) +
        stat_function(fun = dgamma, args = 1,
                      aes(colour = "lambda = 1")) +
        ggtitle("Densidad Exponencial(lambda)") +
        ylab("fx") +
        labs(colour = "Parámetro") + theme_bw()
```

**Propiedades:**

-   $E(Y) = \frac{1}{\lambda}$

-   $Var(Y) = \frac{1}{\lambda^2}$

-   $M_Y(t) = \frac{\lambda}{\lambda -t}$ para $t<\lambda$

-   El estimador por momentos es igual al estimador máximo verosímil de $\lambda$ y es $$\hat{\lambda}_{mm}=\hat{\lambda}_{mv} = \frac{1}{\overline{Y}}$$

**Ejemplo 5**

```{r ej5, echo=TRUE}
n <- 300 #Tamaño de muesta
# Parámetro
lambda <- 6

# Muestra simulada con los parámetros elegidos
set.seed(47)
y <- rexp(n, rate = lambda)

# Estimación de lambda
(lambda_mv <- 1/mean(y))
```

```{r tabla_exp}
kable(data.frame(par=lambda,mv=lambda_mv),
      col.names = c("Valor del parámetro","Estimación"))
```

### Gamma

Si $Y\sim$ Gamma$(\alpha,\lambda)$ entonces, $$f_Y(y) = \frac{(\lambda y)^{\alpha-1}}{\Gamma(\alpha)}\lambda e^{-\lambda y}$$ con $\alpha >0$ y $\lambda > 0$ para $y>0$.

**Propiedades:**

-   $E(Y) = \frac{\alpha}{\lambda}$

-   $Var(Y) = \frac{\alpha}{\lambda^2}$

-   $M_Y(t) = \left(\frac{\lambda}{\lambda -t}\right)^\alpha$ para $t<\lambda$

-   Los estimadores por momentos son: $$\hat{\alpha}_{mm}=\frac{\overline{Y}^2}{\hat{\sigma}^2}$$ $$\hat{\lambda}_{mm} = \frac{\overline{Y}}{\hat{\sigma}^2}$$

-   Cuando $\alpha$ es conocida, el estimador máximo verosímil de $\lambda$ es $$\hat{\lambda}_{mv} = \frac{\alpha}{\overline{Y}}$$

```{r gamma, fig.cap="Función de densidad Gamma con distintos parámetros."}
ggplot(data.frame(x = c(0, 10)), aes(x = x)) +
        stat_function(fun = dgamma, args = list(10, 3),
                      aes(colour = "alpha = 10, lambda = 3")) +
        stat_function(fun = dgamma, args = list(3, 0.8),
                      aes(colour = "alpha = 3, lambda = 0.8")) +
        stat_function(fun = dgamma, args = list(6, 0.7),
                      aes(colour = "alpha = 6, lambda = 0.7")) +
        ggtitle("Densidad Gamma(alpha, lambda)") +
        labs(colour = "Parámetros") + ylab("fx") + theme_bw()
```

```{r,include=FALSE}
data("IndustryAuto")
y <- IndustryAuto$Claim
hist(y)
set.seed(2525)
#y <- rgamma(2000, shape=5, rate=1)
aux <- log(mean(y)) - mean(log(y))
f <- function(x) log(x) - digamma(x) - aux
(alpha <- uniroot(f, c(1e-8,1e8))$root) ## 5.049
(beta <- alpha/mean(y))


#Función de distribución empírica 
F_emp_g <- ecdf(y)

x<-seq(1,50000,by=10) # Para graficar
F_emp_x <- F_emp_g(x)

#Función de distribución ajustada
F_aj_x <- pgamma(x,shape=alpha,rate=beta)

#Función de distribución empírica y distribución con el parámetro estimado
ggplot(data.frame(x,F_emp_x),aes(x=x,y=F_emp_x)) +
  geom_step() +
  geom_line(aes(x=x,y=F_aj_x),color="red") +
  theme_bw()

```

**Ejemplo 6**

Estimación de ambos parámetros con el método de momentos y por máxima verosimilitud usando la función `fitdist`. **Nota:** La distribución Gamma en `R` tiene una parametrización diferente en la que recibe el parámetro `rate` = $\lambda$ o `scale`= $1/\lambda$.

```{r ej6, echo=TRUE}
n <- 1000 #Tamaño de muesta

# Parámetros
alpha <- 6
lambda <- 2

# Muestra simulada con los parámetros elegidos
set.seed(72)
y <- rgamma(n = n,shape = alpha, rate = lambda)

# Estimación por momentos
(alpha_mm <- (mean(y)^2)/(var(y)*(n-1)/n))
(lambda_mm <- mean(y)/(var(y)*(n-1)/n))

# Estimación por máxima verosimilitud
fd_gm <- fitdist(y,distr="gamma",method = "mle")
fd_gm$estimate
```

```{r tabla_gm}
kable(data.frame(par=c(alpha,lambda),mm=c(alpha_mm,lambda_mm),mv=as.numeric(fd_gm$estimate)),
      col.names = c("Valor de los parámetros","Estimación por momentos","Estimación por MV"))
```

### Pareto

Si $Y\sim$ Pareto$(\alpha,\lambda)$ entonces, $$f_Y(y) = \frac{\alpha \lambda^\alpha}{(\alpha+y)^{\alpha +1}}$$ con $\alpha >0$ y $\lambda > 0$ para $y>0$.

**Propiedades:**

-   $E(Y) = \frac{\lambda}{\alpha - 1}$ cuando $\alpha > 1$.

-   $Var(Y) = \frac{\alpha\lambda^2}{(\alpha-1)^2(\alpha-2)}$ cuando $\alpha > 2$.

-   Los estimadores por momentos son: $$\hat{\alpha}_{mm}=\frac{2\hat{\sigma}^2}{\hat{\sigma}^2-\overline{Y}^2}$$ $$\hat{\lambda}_{mm} = \overline{Y}(\hat{\alpha}_{mm}-1)$$

```{r pareto, fig.cap="Función de densidad Pareto con distintos parámetros."}
ggplot(data.frame(x = c(0, 10)), aes(x = x)) +
        stat_function(fun = dpareto, args = list(3, 10),
                      aes(colour = "alpha = 3, lambda = 10")) +
        stat_function(fun = dpareto, args = list(1, 5),
                      aes(colour = "alpha = 1, lambda = 5")) +
        stat_function(fun = dpareto, args = list(4, 9),
                      aes(colour = "alpha = 4, lambda = 9")) +
        ggtitle("Densidad Pareto(alpha, lambda)") +
        labs(colour = "Parámetros") + ylab("fx") + theme_bw()
```

**Ejemplo 7**

Estimación de parámetros (con $\alpha >2$).

```{r ej7, echo=TRUE}
n <- 300 #Tamaño de muesta

# Parámetros
alpha <- 3
lambda <- 5

# Muestra simulada con los parámetros elegidos
set.seed(73)
y <- rpareto(n = n, shape = alpha, scale = lambda)

# Estimación por momentos
(alpha_mm <- (2*var(y)*(n-1)/n)/(var(y)*(n-1)/n - mean(y)^2))
(lambda_mm <- mean(y)*(alpha_mm-1))

# Estimación por máxima verosimilitud
fd_prt <- fitdist(y,distr="pareto",method = "mle")
fd_prt$estimate
```

```{r tabla_prt}
kable(data.frame(par=c(alpha,lambda),mm=c(alpha_mm,lambda_mm),mv=as.numeric(fd_prt$estimate)),
      col.names = c("Valor de los parámetros","Estimación por momentos","Estimación por MV"))
```

### Lognormal

Si $Y\sim$ Lognormal$(\mu,\sigma)$ entonces, $$f_Y(y) = \frac{1}{\sqrt{2\pi \sigma^2}y}e^{-\frac{(log(y) - \mu )^2}{2\sigma^2}}$$ con $\mu \in \mathbb{R}$ y $\sigma^2 > 0$ para $y>0$.

**Propiedades:**

-   $E(Y) = e^{\mu + \frac{\sigma^2}{2}}$

-   $Var(Y) = e^{2\mu + \sigma^2}\cdot e^{\sigma^2-1}$

-   Los estimadores por momentos son: $$\hat{\mu}_{mm}=log\left(\frac{\overline{Y}^2}{\sqrt{\frac{\sum_{i=1}^n{Y_i^2}}{n}}}\right)$$ $$\hat{\sigma}_{mm} = log\left(\frac{\frac{1}{n}\sum_{i=1}^n{Y_i^2}}{\overline{Y}^2}\right)$$

-   Los estimadores por máxima verosimilitud son $$\hat{\mu}_{mv} = \frac{1}{n}\sum_{i=1}^n log(Y_i)$$ $$\hat{\sigma}_{mv}^2 = \frac{1}{n}\sum_{i=1}^n (log(Y_i)-\hat{\mu}_{mv})$$

```{r lognorm, fig.cap="Función de densidad Lognormal con distintos parámetros."}
ggplot(data.frame(x = c(0.08, 4)), aes(x = x)) +
        stat_function(fun = dlnorm, args = list(5, 9),
                      aes(colour = "mu = 5, sigma = 9")) +
        stat_function(fun = dlnorm, args = list(4, 2.5),
                      aes(colour = "mu = 4, sigma = 2.5")) +
        stat_function(fun = dlnorm, args = list(1, 2),
                      aes(colour = "mu = 1, sigma = 2")) +
        ggtitle("Densidad Lognormal(mu, sigma^2)") +
        ylab("fx") +
        labs(colour = "Parámetros") + theme_bw()
```

```{r ej8, echo=TRUE}
n <- 300 #Tamaño de muesta

# Parámetros
mu <- 4
sigma <- 2.5

# Muestra simulada con los parámetros elegidos
set.seed(39)
y <- rlnorm(n,meanlog = mu, sdlog = sigma)

# Estimación por momentos
(mu_mm <- log(mean(y)^2/sqrt(sum(y^2)/n)))
(sigma_mm <- log((sum(y^2)/n)/mean(y)^2))

# Estimación por máxima verosimilitud
#Con la fórmula
(mu_mv <- sum(log(y))/n)
(sigma_mv <- sqrt(sum((log(y)-mu_mv)^2)/n))

#Usando fitdist
fd_lnorm <- fitdist(y,distr="lnorm",method = "mle")
fd_lnorm$estimate
```

```{r tabla_lnorm}
kable(data.frame(par=c(mu,sigma),mm=c(mu_mm,sigma_mm),mv=as.numeric(fd_lnorm$estimate)),
      col.names = c("Valor de los parámetros","Estimación por momentos","Estimación por MV"))
```
